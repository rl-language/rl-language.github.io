.. rlc-doc documentation master file, created by
   sphinx-quickstart on Sun May 11 18:38:45 2025.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Rulebook documentation
======================

**Rulebook** is a imperative language for `complex interactive subsystems <./the_inversion_of_control_problem.html#complex-interactive-subsystems>`_  (reinforcement learning environments, videogames, UIs with graph-like transitions, multistep procedures, ...).

Rulebook is compiled and statically checked, the key and innovative feature of the language are `Action functions with SPIN properties <./language_tour.html#action-functions>`_ , which help to:

* `store, load, print, replay, modify <./language_tour.html#spin-functions-implications>`_ both execution traces and the program state
* **automatically test** your interactive code using off-the-shelf `fuzzers <./language_tour.html#automatic-testing>`_ , `proofs <./language_tour.html#finite-interactive-programs>`_ and `reinforcement learning <./language_tour.html#reinforcement-learning>`_.
* write `self-configuring UIs <./language_tour.html#self-configuring-uis>`_, where UIs can inspect the underlying program they present and configure themselves accordingly.
* `automatically remote execute <./language_tour.html#remote-execution>`_ interactive code over the network.
* solves the `interactive inversion of control problem <./the_inversion_of_control_problem.html>`_ at the language level.

Rulebook:

* **aids**, not replaces `C, C++, C#, Python, and Godot Script <./language_tour.html#compatibility>`_  (just like SQL aids but not replaces those languages)
* produces a single shared library (or webassembly if targeting the web) with the same ABI as C that you can embed in your software, wrapped into generated file native to your language.

Our key proof of concept example is `4Hammer <https://github.com/rl-language/4Hammer>`_ . A never before implemented reinforcement learning environment with huge amounts of user actions in only ~5k lines of code (including graphical code). It runs in the browser and on desktop and all the features described in this section are present.


.. toctree::
   :maxdepth: 2
   :caption: Concepts:

   language_tour.md
   the_inversion_of_control_problem.md
   project_rationale.md


Reinforcement learning usecase
##############################

**Reinforcement learning environments** are often subject to the **interactive inversion of control problem**, since they must be stopped and resumed every time the artificial agent wishes to take an action. The following documents in detail what **Rulebook** does in that domain. In particular, we show:

* how to interoperate **Rulebook** with **Python**.
* how you can write almost arbitrary functions, turn them into a **GYM** environment, and have your custom agents learn from such environments.

.. toctree::
   :maxdepth: 3
   :caption: Architecture 

   architecture.md 

.. toctree::
   :maxdepth: 2
   :caption: Reinforcement Learning Docs:

   tutorial.md
   gym_tutorial.md
   

UI, Controller and Gameplay Programming
########################################

`UI, controller <https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller>`_ and gameplay programming are among the sections that benefit the most from using Rulebook. Beside automatically `fuzz <./language_tour.html#automatic-testing>`_ , `prove correct <./language_tour.html#finite-interactive-programs>`_ and `run reinforcement learning on <./language_tour.html#reinforcement-learning>`_ your program, write  `self-configuring UIs <./language_tour.html#self-configuring-uis>`_ , Rulebook automatically provides a sharp division between UI concerns and Controllers or Gameplay.


.. toctree::
   :maxdepth: 3
   :caption: UIs, Controllers, Gameplay:

   ui_gameplay.md 


.. toctree::
   :maxdepth: 3
   :caption: References:

   language-reference.md 
   stdlib/index.rst



