.. rlc-doc documentation master file, created by
   sphinx-quickstart on Sun May 11 18:38:45 2025.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Rulebook documentation
======================

**Rulebook** is a language for `complex interactive subsystems <./the_inversion_of_control_problem.html#complex-interactive-subsystems>`_  (reinforcement learning environments, videogames, UIs with graph-like transitions, multistep procedures, ...).

Rulebook is compiled and statically checked, the key and innovative feature of the language are `Action functions with SPIN properties <./language_tour.html#action-functions>`_ , which help to:

* **properly separate and reuse** interactive code
* **automatically test** your interactive code using fuzzers and reinforcement learning.
* **automatically trace and reply** interactive code.
* **automatically remote execute** interactive code over the network.
* **write self-configuring UIs**, where UIs can inspect the underlying program they present and configure themselves accordingly.
* solves the `interactive inversion of control problem <./the_inversion_of_control_problem.html>`_ at the language level.

Rulebook:

* **aids**, not replaces C, C++, C#, Python, and Godot Script. (just like SQL aids but not replaces those languages)
* produces a single shared library (or webassembly if targeting the web) with the same ABI as C that you can embed in your software, wrapped into generated file native to your language.

Our key proof of concept example is `4Hammer <https://github.com/rl-language/4Hammer>`_ . A never before implemented reinforcement learning environment with huge amounts of user actions in only ~5k lines of code (including graphical code). It runs in the browser and on desktop and all the features described in this section are are present.


.. toctree::
   :maxdepth: 2
   :caption: Concepts:

   language_tour.md
   the_inversion_of_control_problem.md


Reinforcement learning usecase
##############################

**Reinforcement learning environments** are often subject to the **interactive inversion of control problem**, since they must be stopped and resumed every time the artificial agent wishes to take an action. The following documents in detail what **Rulebook** does in that domain. In particular, we show:

* how to interoperate **Rulebook** with **Python**.
* how you can write almost arbitrary functions, turn them into a **GYM** environment, and have your custom agents learn from such environments.


.. toctree::
   :maxdepth: 2
   :caption: Reinforcement Learning Docs:

   project_rationale.md
   rationale.md
   tutorial.md
   gym_tutorial.md
   

Graphical engines
#################

Graphical engines 


.. toctree::
   :maxdepth: 3
   :caption: References:

   language-reference.md 
   stdlib/index.rst



